Cloud-Based Research Paper Summarization Tool
Project Proposal (AWS + GCP, RAG Architecture)
1.	Introduction
The Cloud-Based Research Paper Summarization Tool aims to simplify how students and researchers understand academic papers. It will allow users to search for research papers from public repositories (e.g., arXiv / Semantic), upload their own PDFs, and interact with them through a chat-like interface that generates accurate, citation-backed summaries and answers.
The system will be implemented as a cross-cloud, scalable web application using AWS for the core web platform and GCP for LLM/RAG workloads, with a strong focus on
open-source/self-hosted components and Retrieval-Augmented Generation (RAG).

2.	Objectives
●	O1 – Search & Retrieval: Allow users to search for papers from external repositories (e.g., arXiv / Semantic) and ingest them into their workspace.
●	O2 – Upload & Summarization: Generate concise, context-grounded summaries of user-uploaded research papers.
●	O3 – Interactive Q&A: Let users interact with uploaded PDFs, asking detailed questions (Google NotebookLM style).
●	O4 – Cross-Cloud Architecture: Use AWS + GCP with clear separation of responsibilities to demonstrate scalability and cloud design skills.
●	O5 – RAG & Vector Search: Implement a RAG pipeline with a vector database for semantic search and context retrieval.
●	O6 – Production-Like Features: Include user authentication, API Gateway, and a
frontend interface to make the system feel production-ready.
●	O7 – Cost-Conscious Design: Prefer open-source/self-hosted tools; allow small-budget paid services only where they clearly add value.


3.	Unique Differentiators (vs a “basic” RAG summarizer)
To make our project stand out from other teams building similar tools, we will add the following unique features:
●	3.1 Multi-Paper Literature Review Mode
○	Users can select multiple papers.
○	System clusters them by topic using vector embeddings.
○	Generates a structured literature review:
■	Key themes and contributions.
 
■	Comparison of methods and results.
■	Identified research gaps and future directions.
○	Each point is citation-backed with references to specific papers and pages.
●	3.2 Persistent Research Workspace (Per User)
○	Authenticated users get a personal “research workspace”.
○	Uploaded/searched PDFs are stored and embedded once; users can return later and continue conversation.
○	Past questions and feedback are stored to provide personalized, context-aware responses.
●	3.3 Citation-First Conversational Interface
○	Chat UI where every answer includes:
■	Relevant text snippets from papers.
■	Paper title, section, and page number.
○	Supports different answer modes:
■	“Quick summary” (abstract-style).
■	“Deep explanation” (exam-style with step-by-step reasoning).
●	3.4 Cross-Cloud RAG Story
○	AWS: Web frontend, API, authentication, databases, secure secrets.
○	GCP: LLM inference, vector search, and heavy PDF processing.
○	Explicit discussion of why each cloud is chosen and how the design supports
scalability and fault isolation.
●	3.5 Feedback-Driven Quality Improvement
○	Users can rate answers (thumbs up/down, relevance score).
○	Feedback is stored and (as a stretch goal) used to:
■	Adjust retrieval thresholds.
■	Re-rank chunks.




4.	High-Level System Architecture
4.1	Overall Flow
1.	User actions (search, upload PDF, ask questions) happen via a web frontend hosted on AWS.
2.	Requests go through an API Gateway into backend microservices (AWS).
3.	When RAG/LLM processing is needed, the backend calls GCP services:
○	PDF text extraction and embedding.
○	Vector storage and semantic search.
○	LLM for summarization and question answering.
4.	GCP returns relevant passages and model responses to the AWS backend.
5.	AWS backend integrates results, attaches citations, stores history, and responds to the frontend.
 
 
5.	Cloud Architecture Breakdown
5.1	AWS Components (Platform & Data Layer)
●	Frontend Hosting
○	AWS S3 + CloudFront for hosting the single-page web app (React or similar).
●	API Layer
○	Amazon API Gateway to expose REST/HTTP APIs.
○	AWS Lambda or ECS/Fargate to run backend services (e.g., Python FastAPI or Node.js).
○	Amazon SQS (Simple Queue Service) to decouple PDF ingestion, making it asynchronous and resilient.
●	Authentication & Security
○	Amazon Cognito for user sign-up/sign-in and JWT-based API authentication.
○	AWS Secrets Manager to securely store GCP service account keys and other API keys (e.g., for arXiv).
●	Databases
○	Amazon RDS (e.g., PostgreSQL) for relational data: Users, conversations, feedback, and document metadata.
○	Amazon DynamoDB for fast-access, non-relational data: Document processing status, session state, or per-chunk metadata.
●	Object Storage
○	Amazon S3 for storing user-uploaded PDFs and derived text artifacts.



5.2	GCP Components (RAG, Vector DB, and LLM Layer)
●	Compute for RAG Services
○	Google Cloud Run or GKE to host containerized microservices for:
1.	PDF processing worker (text extraction, chunking).
2.	Embedding worker (using open-source models).
3.	RAG service for retrieval and aggregation.
●	Vector Database (Open-Source Preferred)
○	Example: Qdrant or Weaviate deployed on GCP (via Cloud Run/GKE or managed marketplace).
○	Stores document embeddings and allows similarity search, filtered by user or document ID.
●	LLM Inference
○	Two options (hybrid approach):
1.	Open-source/self-hosted LLM (e.g., Llama 3 variant) served on GCP compute (GKE/GCE with GPU).
2.	Vertex AI Models (e.g., Gemini) as a pay-per-use endpoint, which the system can switch to for higher-quality generation.
●	Inter-Cloud Communication
 
○	AWS backend calls GCP endpoints over HTTPS.
○	Authentication is handled by GCP service accounts, with keys securely retrieved from AWS Secrets Manager by the AWS backend. This avoids hard-coding keys.



6.	RAG (Retrieval-Augmented Generation) Pipeline
6.1	Ingestion Pipeline 1 (External Search)
1.	Search Query: User enters a search term in the frontend (e.g., "RAG models").
2.	API Call (AWS): AWS backend service (Lambda) retrieves the arXiv/Semantic Scholar API key from Secrets Manager.
3.	External Fetch: Lambda calls the external API, gets a list of papers, and returns it to the frontend.
4.	User Selection: User clicks "Add to Workspace" on a paper.
5.	Backend Download: AWS backend service downloads the paper's PDF from the external source.
6.	Trigger Ingestion: The backend uploads this new PDF to S3, which then follows the
Ingestion Pipeline 2 (User Upload) flow below.

6.2	Ingestion Pipeline 2 (User Upload)
1.	Upload: User uploads a PDF through the web app.
2.	Store & Trigger: File is stored in S3. An S3 Event Notification triggers an AWS Lambda function (or places a message in an SQS queue).
3.	Notify GCP: The AWS Lambda function (or a worker processing the SQS message) calls the GCP processing endpoint, providing the S3 bucket and file key.
4.	PDF Processing (GCP): GCP worker downloads the PDF from S3, extracts text, cleans it, and splits it into semantic chunks. (Note: This incurs AWS S3 egress costs).
5.	Embedding (GCP): Each chunk is converted into a vector embedding (e.g., using all-MiniLM-L6-v2).
6.	Indexing (GCP): Embeddings + metadata (user ID, document ID, page number) are stored in the vector database.
7.	Update Status: The GCP service notifies the AWS backend (e.g., via a webhook or API call) to update the document's status in DynamoDB/RDS to "Ready."

6.3	Query Pipeline (When User Asks a Question)
1.	User Query: Authenticated user selects documents and asks a question.
2.	Backend Handling (AWS): Backend checks permissions and forwards the query to the GCP RAG service.
3.	Retrieval (GCP): Vector DB performs similarity search to retrieve the top relevant chunks.
 
4.	Response Generation (GCP): Retrieved chunks + question are passed to the LLM (e.g., Llama-3-8B-Instruct).
5.	Return & Storage (AWS): The answer + citations are returned to the AWS backend, stored in RDS, and sent to the frontend.


7.	Data Model (High-Level)
●	Users: user_id, email, cognito_sub, created_at
●	Documents: document_id, user_id (owner), title, source_url, s3_path, status (processing, ready, error), created_at
●	Chunks (Metadata): chunk_id, document_id, page_number, vector_id
●	Conversations: conversation_id, user_id, title, created_at
●	Messages: message_id, conversation_id, sender (user/system), content, citations (JSON), created_at
●	Feedback: feedback_id, message_id, user_id, rating (int), comments


8.	Technology Stack (Tentative)
●	Frontend: React (hosted on AWS S3 + CloudFront).
●	Backend: Python + FastAPI (hosted on AWS Lambda or ECS/Fargate).
●	API: Amazon API Gateway.
●	RAG & LLM Services (GCP): Containerized Python services (on Cloud Run or GKE).
●	Vector DB: Qdrant / Weaviate (self-hosted on GCP).
●	Embeddings & LLM:
○	Embeddings: all-MiniLM-L6-v2 or BGE-M3 (open-source).
○	LLM: Llama-3-8B-Instruct or Phi-3-mini (open-source); optional Vertex AI Gemini.
●	Databases: Amazon RDS (PostgreSQL) & Amazon DynamoDB.
●	Security & Events: Amazon Cognito, AWS Secrets Manager, Amazon SQS.


9.	Scalability and Reliability Considerations
●	Stateless Microservices: Backend and RAG microservices are stateless and can be scaled horizontally.
●	Separation of Concerns: AWS handles stateful user data; GCP handles heavy, bursty compute.
●	Event-Driven Processing: Using SQS for ingestion means uploads are non-blocking. If processing fails, messages can be retried, increasing reliability.
●	Caching: API Gateway and CloudFront can cache static assets and common API responses.


 
10.	Development Milestones
●	Phase 1 – Core MVP (Upload RAG)
○	Basic UI with PDF upload.
○	End-to-end RAG pipeline (Ingestion 2 & Query) for a single document.
○	Core AWS (S3, Lambda, API-GW) + GCP (Cloud Run, Vector DB) working.
●	Phase 2 – Search, Auth & Workspace
○	Implement User Authentication via Cognito.
○	Implement Persistent Workspace (saved documents/conversations in RDS).
○	Implement Search Ingestion (Pipeline 1) for arXiv.
●	Phase 3 – Unique Differentiator
○	Implement Multi-Paper selection in the UI.
○	Develop the Literature Review Mode (clustering and synthesis).
○	Implement Citation-First UI to show page numbers.
●	Phase 4 – Optimization & Polish
○	Implement the Feedback System (storing user ratings).
○	UI polish, loading states, error handling.
○	Cost/performance optimization review.



